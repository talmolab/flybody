{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Current Issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When swapping the agent to use the vision, it dislike the observation space where it uses the camera, since the rendering output of the camera is `('walker/egocentric_camera', BoundedArray(shape=(64, 64, 3), dtype=dtype('uint8')`, but it expect a float."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Fixed! This is achieved by the new wrapper that converts the uint8 to float32, to be consistent with the rest of the observation spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> New Error\n",
    "\n",
    "```python\n",
    "2024-07-19 07:59:43,188 ERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): ray::EnvironmentLoop.run() (pid=2623445, ip=10.244.10.88, actor_id=7619541ce6bdcfe57077c81e01000000, repr=<flybody.agents.ray_distributed_dmpo.EnvironmentLoop object at 0x7f1883744be0>)\n",
    "  File \"/root/anaconda3/envs/flybody/lib/python3.10/site-packages/acme/environment_loop.py\", line 158, in run\n",
    "    result = self.run_episode()\n",
    "  File \"/root/anaconda3/envs/flybody/lib/python3.10/site-packages/acme/environment_loop.py\", line 94, in run_episode\n",
    "    action = self._actor.select_action(timestep.observation)\n",
    "  File \"/root/vast/scott-yang/flybody/flybody/agents/actors.py\", line 79, in select_action\n",
    "    action = self._policy(observation)\n",
    "  File \"/root/anaconda3/envs/flybody/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\n",
    "    raise e.with_traceback(filtered_tb) from None\n",
    "  File \"/root/anaconda3/envs/flybody/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py\", line 1147, in autograph_handler\n",
    "    raise e.ag_error_metadata.to_exception(e)\n",
    "TypeError: in user code:\n",
    "\n",
    "    File \"/root/vast/scott-yang/flybody/flybody/agents/actors.py\", line 65, in _policy  *\n",
    "        policy = self._policy_network(batched_observation)\n",
    "    File \"/root/anaconda3/envs/flybody/lib/python3.10/site-packages/sonnet/src/utils.py\", line 85, in _decorate_unbound_method  *\n",
    "        return decorator_fn(bound_method, self, args, kwargs)\n",
    "    File \"/root/anaconda3/envs/flybody/lib/python3.10/site-packages/sonnet/src/base.py\", line 262, in wrap_with_name_scope  *\n",
    "        return method(*args, **kwargs)\n",
    "    File \"/root/anaconda3/envs/flybody/lib/python3.10/site-packages/sonnet/src/sequential.py\", line 68, in __call__  *\n",
    "        outputs = mod(outputs, *args, **kwargs)\n",
    "    File \"/root/anaconda3/envs/flybody/lib/python3.10/site-packages/sonnet/src/utils.py\", line 85, in _decorate_unbound_method  *\n",
    "        return decorator_fn(bound_method, self, args, kwargs)\n",
    "    File \"/root/anaconda3/envs/flybody/lib/python3.10/site-packages/sonnet/src/base.py\", line 262, in wrap_with_name_scope  *\n",
    "        return method(*args, **kwargs)\n",
    "    File \"/root/anaconda3/envs/flybody/lib/python3.10/site-packages/acme/tf/utils.py\", line 144, in __call__  *\n",
    "        return self._transformation(*args, **kwargs)\n",
    "    File \"/root/anaconda3/envs/flybody/lib/python3.10/site-packages/acme/tf/utils.py\", line 54, in batch_concat  *\n",
    "        return tf.concat(tree.flatten(flat_leaves), axis=-1)\n",
    "\n",
    "    TypeError: Tensors in list passed to 'values' of 'ConcatV2' Op have types [float32, float32, float32, uint8, float32, float32, float32, float32, float32, float32, float32, float32, float32, float32, float32, float32] that don't all match.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Issue (as of Jul 19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- two touch and maze forage is not training. \n",
    "- Get those enviornment training!\n",
    "- seems like some enviornment have integer task logic gate, and the neural network is not happy.\n",
    "\n",
    "- TODO: wandb logging with ray,\n",
    "- TODO: ask Salk support for Ray distributed training.\n",
    "- maybe 0.4 gpu for training\n",
    "\n",
    "\n",
    "\n",
    "> - TODO: environment factory is a function that is being passed into the env loop. So we need functool.partial for this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Jul 25 Issue: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When trying to train a generalist, we found that\n",
    "```python\n",
    "(Learner pid=623126) None\n",
    "Error executing job with overrides: []\n",
    "Traceback (most recent call last):\n",
    "  File \"python/ray/_raylet.pyx\", line 919, in ray._raylet.prepare_args_internal\n",
    "  File \"/root/anaconda3/envs/flybody/lib/python3.10/site-packages/ray/_private/serialization.py\", line 519, in serialize\n",
    "    return self._serialize_to_msgpack(value)\n",
    "  File \"/root/anaconda3/envs/flybody/lib/python3.10/site-packages/ray/_private/serialization.py\", line 497, in _serialize_to_msgpack\n",
    "    pickle5_serialized_object = self._serialize_to_pickle5(\n",
    "  File \"/root/anaconda3/envs/flybody/lib/python3.10/site-packages/ray/_private/serialization.py\", line 444, in _serialize_to_pickle5\n",
    "    raise e\n",
    "  File \"/root/anaconda3/envs/flybody/lib/python3.10/site-packages/ray/_private/serialization.py\", line 439, in _serialize_to_pickle5\n",
    "    inband = pickle.dumps(\n",
    "  File \"/root/anaconda3/envs/flybody/lib/python3.10/site-packages/ray/cloudpickle/cloudpickle.py\", line 1479, in dumps\n",
    "    cp.dump(obj)\n",
    "  File \"/root/anaconda3/envs/flybody/lib/python3.10/site-packages/ray/cloudpickle/cloudpickle.py\", line 1245, in dump\n",
    "    return super().dump(obj)\n",
    "TypeError: cannot pickle 'weakref.ReferenceType' object\n",
    "\n",
    "The above exception was the direct cause of the following exception:\n",
    "\n",
    "Traceback (most recent call last):\n",
    "  File \"/root/vast/scott-yang/flybody/flybody/train_dmpo_ray.py\", line 262, in main\n",
    "    actors.append(create_actors(num_actors, environment_factories[name]))\n",
    "  File \"/root/vast/scott-yang/flybody/flybody/train_dmpo_ray.py\", line 243, in create_actors\n",
    "    actor = EnvironmentLoop.remote(\n",
    "  File \"/root/anaconda3/envs/flybody/lib/python3.10/site-packages/ray/actor.py\", line 718, in remote\n",
    "    return self._remote(args=args, kwargs=kwargs, **self._default_options)\n",
    "  File \"/root/anaconda3/envs/flybody/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
    "    return fn(*args, **kwargs)\n",
    "  File \"/root/anaconda3/envs/flybody/lib/python3.10/site-packages/ray/util/tracing/tracing_helper.py\", line 388, in _invocation_actor_class_remote_span\n",
    "    return method(self, args, kwargs, *_args, **_kwargs)\n",
    "  File \"/root/anaconda3/envs/flybody/lib/python3.10/site-packages/ray/actor.py\", line 1161, in _remote\n",
    "    actor_id = worker.core_worker.create_actor(\n",
    "  File \"python/ray/_raylet.pyx\", line 4111, in ray._raylet.CoreWorker.create_actor\n",
    "  File \"python/ray/_raylet.pyx\", line 4116, in ray._raylet.CoreWorker.create_actor\n",
    "  File \"python/ray/_raylet.pyx\", line 878, in ray._raylet.prepare_args_and_increment_put_refs\n",
    "  File \"python/ray/_raylet.pyx\", line 869, in ray._raylet.prepare_args_and_increment_put_refs\n",
    "  File \"python/ray/_raylet.pyx\", line 928, in ray._raylet.prepare_args_internal\n",
    "TypeError: Could not serialize the argument <acme.wrappers.canonical_spec.CanonicalSpecWrapper object at 0x7f4aac1ea7a0> for a task or actor flybody.agents.ray_distributed_dmpo.EnvironmentLoop.__init__:\n",
    "================================================================================\n",
    "Checking Serializability of <acme.wrappers.canonical_spec.CanonicalSpecWrapper object at 0x7f4aac1ea7a0>\n",
    "================================================================================\n",
    "!!! FAIL serialization: cannot pickle 'weakref.ReferenceType' object\n",
    "    Serializing '_abc_impl' <_abc._abc_data object at 0x7f50ec2f4480>...\n",
    "    !!! FAIL serialization: cannot pickle '_abc._abc_data' object\n",
    "    WARNING: Did not find non-serializable object in <_abc._abc_data object at 0x7f50ec2f4480>. This may be an oversight.\n",
    "================================================================================\n",
    "Variable: \n",
    "\n",
    "        FailTuple(_abc_impl [obj=<_abc._abc_data object at 0x7f50ec2f4480>, parent=<acme.wrappers.canonical_spec.CanonicalSpecWrapper object at 0x7f4aac1ea7a0>])\n",
    "\n",
    "was found to be non-serializable. There may be multiple other undetected variables that were non-serializable. \n",
    "Consider either removing the instantiation/imports of these variables or moving the instantiation into the scope of the function/class. \n",
    "================================================================================\n",
    "Check https://docs.ray.io/en/master/ray-core/objects/serialization.html#troubleshooting for more information.\n",
    "If you have any suggestions on how to improve this error message, please reach out to the Ray developers on github.com/ray-project/ray/issues/\n",
    "================================================================================\n",
    "```\n",
    "\n",
    "Might due to my environment factory function is not serializable. Might need to write multiple function to adapt to this scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# July 26 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task switching works, need to align observation spaces between task, it won't be a issue for the decoder encoder arch, where we shared the decoder but not the encoder.\n",
    "\n",
    "**Observation Space Differences:**\n",
    "\n",
    "- Run Gaps \n",
    "    - (base)\n",
    "- Bowl Escape \n",
    "    - (base + origin coordinates)\n",
    "- Maze Forage \n",
    "    - (base)\n",
    "- Two Taps \n",
    "    - (base + task logic)\n",
    "\n",
    "To enable joint training, we need to modify the observation space of all task to be the same.\n",
    "\n",
    "_Question: At which layer does can we efficiently and reusably inject this observation change?_\n",
    "\n",
    "> **(1):** Change in network (easy to implement but hard to load the checkpoint) \n",
    ">\n",
    "> **(2):** Change the definition of tasks (more invovled implementation but it is a one time for all change)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# July 29 Monday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Progress:_\n",
    "\n",
    "make sure the observation spaces between tasks can be interchangabley run.\n",
    "\n",
    "Currently training the bowl escape model, and then train genearlist after that.\n",
    "\n",
    "> Resource Allocation for each node: Head node should have 250G+ RAM, worker should have 60G+ RAM\n",
    "\n",
    "### _Modifications:_\n",
    "\n",
    "I modified the starting location for the rodent for bowl scape, lifting the z axis. If using default spawning location of (0,0,0), the rodent penetrated the bowl, and can be unstable.\n",
    "\n",
    "### _Todos:_\n",
    "\n",
    "- Tuesday Morning I should load the checkpoint, and then render the checkpoint on different tasks.\n",
    "- Ask Salk IT about ethernet connection and the run.ai IP address subnet\n",
    "- Ask Salk IT about networking between node, speed, and potential bottleneck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# July 30 Tuesday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Papers: Kickstarting DRL_\n",
    "\n",
    "- Uses teacher policy, added auxuliry loss term that compares the output of the teacher policy and thes student policy, provide a dense learning schedule.\n",
    "    - Related to Imitation Learning and Skills Transfer.\n",
    "- Teacher also generate an action distributions, as the current student, and compare between two policies. \n",
    "\n",
    "> To do this, they add a distillation loss term to the reinforcement learning loss function, encouraging the student policy to be close to the teacher policy. The weight of this distillation loss is adjusted over time, allowing the student to gradually focus more on maximizing its own rewards and potentially surpass the teacher.\n",
    "\n",
    "_To Implement the KickStarting in this Pipeline:_\n",
    "\n",
    "- Entry Point `DMPOConfig` -> `DMPOBuilder.make_learner` -> `DistributionalMPOLearner` -> adding loss term for _distillation loss_\n",
    "- Maybe inherit another policy loss module, that include the task transfer\n",
    "- Strategy: LowLevel to High Level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# July 31st Wednesday\n",
    "### _Tasks_\n",
    "1. Implement Kickstarting\n",
    "2. Read Papers about neuroscience experiments on vision network -- build analysis pipeline for the model.\n",
    "\n",
    "### _Challenges_:\n",
    "\n",
    "I tried to pass the teacher policy direclty to the loss module, but it fails to serialize the load check point object. So I might need to pass in a string path name for the learner, and ask learner to load the checkpoint.\n",
    "\n",
    "> issue: trying to serialize the read object in the loss module, since we directly provide the loss module from the `DMPOConfig` object. \n",
    "\n",
    "### _Notes:_\n",
    "\n",
    "Since the experience in the replay buffer is a batched tensor that has been run through the observation network (i.e. not a ), "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aug 2nd Friday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Tasks_\n",
    "1. Debugging Kickstarting\n",
    "    - Train the task on two task instead of four.\n",
    "2. Rendering\n",
    "    - Add wandb rendering support for evaluators\n",
    "3. Read Paper\n",
    "\n",
    "### _Notes:_\n",
    "\n",
    "With 240 actors, I can get ~1k4 sps (5.8 sps for single actor) for the actor steps, 44 sps for learner steps, \n",
    "\n",
    "### _Thoughts_\n",
    "\n",
    "During interleave training, the learner is getting experience from the reply buffer in a batched manner. I am not sure whether this is a good idea for multi-task training. The setup is a little bit different compared with CoMic style, since the task switching allow the learner to focus on single task during the gradient step.\n",
    "\n",
    "During joint training, the model seems to inevitably focus on a single task but not the other task. The ReplayBuffer now is a mixed buffer with experiences between tasks. I don't know how that affects the model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Rendering of Generalist - RunGaps_\n",
    "\n",
    "The following is trained by loading the checkpoint of the bowl escape, and run joint training on all four tasks.\n",
    "\n",
    "Definitely interesting that by loading the checkpoint from the bowl escape, the is walking on its foot but not the back. But it lost the ability to do jumps. I also notice that the rodent is awared of the exisitence of the gap, and it is staying on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"/root/vast/scott-yang/flybody/docs/videos/joint_training/run_gaps_genearlist_policy_80.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "Video(\"/root/vast/scott-yang/flybody/docs/videos/joint_training/run_gaps_genearlist_policy_80.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meanwhile, the same model on rodent run gaps environments does not do well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"/root/vast/scott-yang/flybody/docs/videos/joint_training/bowl_escape_genearlist_policy_80.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"/root/vast/scott-yang/flybody/docs/videos/joint_training/bowl_escape_genearlist_policy_80.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rodent is very actively exploring/moving for the rest of the tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"/root/vast/scott-yang/flybody/docs/videos/joint_training/two_touch_genearlist_policy_80.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"/root/vast/scott-yang/flybody/docs/videos/joint_training/two_touch_genearlist_policy_80.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"/root/vast/scott-yang/flybody/docs/videos/joint_training/maze_forage_genearlist_policy_80.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"/root/vast/scott-yang/flybody/docs/videos/joint_training/maze_forage_genearlist_policy_80.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bowl Escape Checkpoint on Four Tasks\n",
    "\n",
    "Interestingly, when directly loading the checkpoints of bowl escape to perform all other tasks, it exhibits stereotypical behavior of walking and exploring, and casually hitting the goal some time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Tasks:_\n",
    "\n",
    "1. Implement multiple reply buffers for each tasks, and the learner of the gradient steps asks for experience in batch but in pure tasks.\n",
    "    - Helps a lot when doing the gradient updates, to preserves the gradient pureity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flybody",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
