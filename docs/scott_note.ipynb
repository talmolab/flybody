{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Current Issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When swapping the agent to use the vision, it dislike the observation space where it uses the camera, since the rendering output of the camera is `('walker/egocentric_camera', BoundedArray(shape=(64, 64, 3), dtype=dtype('uint8')`, but it expect a float."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Fixed! This is achieved by the new wrapper that converts the uint8 to float32, to be consistent with the rest of the observation spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> New Error\n",
    "\n",
    "```python\n",
    "2024-07-19 07:59:43,188 ERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): ray::EnvironmentLoop.run() (pid=2623445, ip=10.244.10.88, actor_id=7619541ce6bdcfe57077c81e01000000, repr=<flybody.agents.ray_distributed_dmpo.EnvironmentLoop object at 0x7f1883744be0>)\n",
    "  File \"/root/anaconda3/envs/flybody/lib/python3.10/site-packages/acme/environment_loop.py\", line 158, in run\n",
    "    result = self.run_episode()\n",
    "  File \"/root/anaconda3/envs/flybody/lib/python3.10/site-packages/acme/environment_loop.py\", line 94, in run_episode\n",
    "    action = self._actor.select_action(timestep.observation)\n",
    "  File \"/root/vast/scott-yang/flybody/flybody/agents/actors.py\", line 79, in select_action\n",
    "    action = self._policy(observation)\n",
    "  File \"/root/anaconda3/envs/flybody/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\n",
    "    raise e.with_traceback(filtered_tb) from None\n",
    "  File \"/root/anaconda3/envs/flybody/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py\", line 1147, in autograph_handler\n",
    "    raise e.ag_error_metadata.to_exception(e)\n",
    "TypeError: in user code:\n",
    "\n",
    "    File \"/root/vast/scott-yang/flybody/flybody/agents/actors.py\", line 65, in _policy  *\n",
    "        policy = self._policy_network(batched_observation)\n",
    "    File \"/root/anaconda3/envs/flybody/lib/python3.10/site-packages/sonnet/src/utils.py\", line 85, in _decorate_unbound_method  *\n",
    "        return decorator_fn(bound_method, self, args, kwargs)\n",
    "    File \"/root/anaconda3/envs/flybody/lib/python3.10/site-packages/sonnet/src/base.py\", line 262, in wrap_with_name_scope  *\n",
    "        return method(*args, **kwargs)\n",
    "    File \"/root/anaconda3/envs/flybody/lib/python3.10/site-packages/sonnet/src/sequential.py\", line 68, in __call__  *\n",
    "        outputs = mod(outputs, *args, **kwargs)\n",
    "    File \"/root/anaconda3/envs/flybody/lib/python3.10/site-packages/sonnet/src/utils.py\", line 85, in _decorate_unbound_method  *\n",
    "        return decorator_fn(bound_method, self, args, kwargs)\n",
    "    File \"/root/anaconda3/envs/flybody/lib/python3.10/site-packages/sonnet/src/base.py\", line 262, in wrap_with_name_scope  *\n",
    "        return method(*args, **kwargs)\n",
    "    File \"/root/anaconda3/envs/flybody/lib/python3.10/site-packages/acme/tf/utils.py\", line 144, in __call__  *\n",
    "        return self._transformation(*args, **kwargs)\n",
    "    File \"/root/anaconda3/envs/flybody/lib/python3.10/site-packages/acme/tf/utils.py\", line 54, in batch_concat  *\n",
    "        return tf.concat(tree.flatten(flat_leaves), axis=-1)\n",
    "\n",
    "    TypeError: Tensors in list passed to 'values' of 'ConcatV2' Op have types [float32, float32, float32, uint8, float32, float32, float32, float32, float32, float32, float32, float32, float32, float32, float32, float32] that don't all match.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Issue (as of Jul 19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- two touch and maze forage is not training. \n",
    "- Get those enviornment training!\n",
    "- seems like some enviornment have integer task logic gate, and the neural network is not happy.\n",
    "\n",
    "- TODO: wandb logging with ray,\n",
    "- TODO: ask Salk support for Ray distributed training.\n",
    "- maybe 0.4 gpu for training\n",
    "\n",
    "\n",
    "\n",
    "> - TODO: environment factory is a function that is being passed into the env loop. So we need functool.partial for this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Jul 25 Issue: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When trying to train a generalist, we found that\n",
    "```python\n",
    "(Learner pid=623126) None\n",
    "Error executing job with overrides: []\n",
    "Traceback (most recent call last):\n",
    "  File \"python/ray/_raylet.pyx\", line 919, in ray._raylet.prepare_args_internal\n",
    "  File \"/root/anaconda3/envs/flybody/lib/python3.10/site-packages/ray/_private/serialization.py\", line 519, in serialize\n",
    "    return self._serialize_to_msgpack(value)\n",
    "  File \"/root/anaconda3/envs/flybody/lib/python3.10/site-packages/ray/_private/serialization.py\", line 497, in _serialize_to_msgpack\n",
    "    pickle5_serialized_object = self._serialize_to_pickle5(\n",
    "  File \"/root/anaconda3/envs/flybody/lib/python3.10/site-packages/ray/_private/serialization.py\", line 444, in _serialize_to_pickle5\n",
    "    raise e\n",
    "  File \"/root/anaconda3/envs/flybody/lib/python3.10/site-packages/ray/_private/serialization.py\", line 439, in _serialize_to_pickle5\n",
    "    inband = pickle.dumps(\n",
    "  File \"/root/anaconda3/envs/flybody/lib/python3.10/site-packages/ray/cloudpickle/cloudpickle.py\", line 1479, in dumps\n",
    "    cp.dump(obj)\n",
    "  File \"/root/anaconda3/envs/flybody/lib/python3.10/site-packages/ray/cloudpickle/cloudpickle.py\", line 1245, in dump\n",
    "    return super().dump(obj)\n",
    "TypeError: cannot pickle 'weakref.ReferenceType' object\n",
    "\n",
    "The above exception was the direct cause of the following exception:\n",
    "\n",
    "Traceback (most recent call last):\n",
    "  File \"/root/vast/scott-yang/flybody/flybody/train_dmpo_ray.py\", line 262, in main\n",
    "    actors.append(create_actors(num_actors, environment_factories[name]))\n",
    "  File \"/root/vast/scott-yang/flybody/flybody/train_dmpo_ray.py\", line 243, in create_actors\n",
    "    actor = EnvironmentLoop.remote(\n",
    "  File \"/root/anaconda3/envs/flybody/lib/python3.10/site-packages/ray/actor.py\", line 718, in remote\n",
    "    return self._remote(args=args, kwargs=kwargs, **self._default_options)\n",
    "  File \"/root/anaconda3/envs/flybody/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
    "    return fn(*args, **kwargs)\n",
    "  File \"/root/anaconda3/envs/flybody/lib/python3.10/site-packages/ray/util/tracing/tracing_helper.py\", line 388, in _invocation_actor_class_remote_span\n",
    "    return method(self, args, kwargs, *_args, **_kwargs)\n",
    "  File \"/root/anaconda3/envs/flybody/lib/python3.10/site-packages/ray/actor.py\", line 1161, in _remote\n",
    "    actor_id = worker.core_worker.create_actor(\n",
    "  File \"python/ray/_raylet.pyx\", line 4111, in ray._raylet.CoreWorker.create_actor\n",
    "  File \"python/ray/_raylet.pyx\", line 4116, in ray._raylet.CoreWorker.create_actor\n",
    "  File \"python/ray/_raylet.pyx\", line 878, in ray._raylet.prepare_args_and_increment_put_refs\n",
    "  File \"python/ray/_raylet.pyx\", line 869, in ray._raylet.prepare_args_and_increment_put_refs\n",
    "  File \"python/ray/_raylet.pyx\", line 928, in ray._raylet.prepare_args_internal\n",
    "TypeError: Could not serialize the argument <acme.wrappers.canonical_spec.CanonicalSpecWrapper object at 0x7f4aac1ea7a0> for a task or actor flybody.agents.ray_distributed_dmpo.EnvironmentLoop.__init__:\n",
    "================================================================================\n",
    "Checking Serializability of <acme.wrappers.canonical_spec.CanonicalSpecWrapper object at 0x7f4aac1ea7a0>\n",
    "================================================================================\n",
    "!!! FAIL serialization: cannot pickle 'weakref.ReferenceType' object\n",
    "    Serializing '_abc_impl' <_abc._abc_data object at 0x7f50ec2f4480>...\n",
    "    !!! FAIL serialization: cannot pickle '_abc._abc_data' object\n",
    "    WARNING: Did not find non-serializable object in <_abc._abc_data object at 0x7f50ec2f4480>. This may be an oversight.\n",
    "================================================================================\n",
    "Variable: \n",
    "\n",
    "        FailTuple(_abc_impl [obj=<_abc._abc_data object at 0x7f50ec2f4480>, parent=<acme.wrappers.canonical_spec.CanonicalSpecWrapper object at 0x7f4aac1ea7a0>])\n",
    "\n",
    "was found to be non-serializable. There may be multiple other undetected variables that were non-serializable. \n",
    "Consider either removing the instantiation/imports of these variables or moving the instantiation into the scope of the function/class. \n",
    "================================================================================\n",
    "Check https://docs.ray.io/en/master/ray-core/objects/serialization.html#troubleshooting for more information.\n",
    "If you have any suggestions on how to improve this error message, please reach out to the Ray developers on github.com/ray-project/ray/issues/\n",
    "================================================================================\n",
    "```\n",
    "\n",
    "Might due to my environment factory function is not serializable. Might need to write multiple function to adapt to this scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# July 26 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task switching works, need to align observation spaces between task, it won't be a issue for the decoder encoder arch, where we shared the decoder but not the encoder.\n",
    "\n",
    "**Observation Space Differences:**\n",
    "\n",
    "- Run Gaps \n",
    "    - (base)\n",
    "- Bowl Escape \n",
    "    - (base + origin coordinates)\n",
    "- Maze Forage \n",
    "    - (base)\n",
    "- Two Taps \n",
    "    - (base + task logic)\n",
    "\n",
    "To enable joint training, we need to modify the observation space of all task to be the same.\n",
    "\n",
    "_Question: At which layer does can we efficiently and reusably inject this observation change?_\n",
    "\n",
    "> **(1):** Change in network (easy to implement but hard to load the checkpoint) \n",
    ">\n",
    "> **(2):** Change the definition of tasks (more invovled implementation but it is a one time for all change)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# July 29 Monday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Progress:_\n",
    "\n",
    "make sure the observation spaces between tasks can be interchangabley run.\n",
    "\n",
    "Currently training the bowl escape model, and then train genearlist after that.\n",
    "\n",
    "> Resource Allocation for each node: Head node should have 250G+ RAM, worker should have 60G+ RAM\n",
    "\n",
    "### _Modifications:_\n",
    "\n",
    "I modified the starting location for the rodent for bowl scape, lifting the z axis. If using default spawning location of (0,0,0), the rodent penetrated the bowl, and can be unstable.\n",
    "\n",
    "### _Todos:_\n",
    "\n",
    "- Tuesday Morning I should load the checkpoint, and then render the checkpoint on different tasks.\n",
    "- Ask Salk IT about ethernet connection and the run.ai IP address subnet\n",
    "- Ask Salk IT about networking between node, speed, and potential bottleneck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# July 30 Tuesday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Papers: Kickstarting DRL_\n",
    "\n",
    "- Uses teacher policy, added auxuliry loss term that compares the output of the teacher policy and thes student policy, provide a dense learning schedule.\n",
    "    - Related to Imitation Learning and Skills Transfer.\n",
    "- Teacher also generate an action distributions, as the current student, and compare between two policies. \n",
    "\n",
    "> To do this, they add a distillation loss term to the reinforcement learning loss function, encouraging the student policy to be close to the teacher policy. The weight of this distillation loss is adjusted over time, allowing the student to gradually focus more on maximizing its own rewards and potentially surpass the teacher.\n",
    "\n",
    "_To Implement the KickStarting in this Pipeline:_\n",
    "\n",
    "- Entry Point `DMPOConfig` -> `DMPOBuilder.make_learner` -> `DistributionalMPOLearner` -> adding loss term for _distillation loss_\n",
    "- Maybe inherit another policy loss module, that include the task transfer\n",
    "- Strategy: LowLevel to High Level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# July 31st Wednesday\n",
    "### _Tasks_\n",
    "1. Implement Kickstarting\n",
    "2. Read Papers about neuroscience experiments on vision network -- build analysis pipeline for the model.\n",
    "\n",
    "### _Challenges_:\n",
    "\n",
    "I tried to pass the teacher policy direclty to the loss module, but it fails to serialize the load check point object. So I might need to pass in a string path name for the learner, and ask learner to load the checkpoint.\n",
    "\n",
    "> issue: trying to serialize the read object in the loss module, since we directly provide the loss module from the `DMPOConfig` object. \n",
    "\n",
    "### _Notes:_\n",
    "\n",
    "Since the experience in the replay buffer is a batched tensor that has been run through the observation network (i.e. not a ), "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aug 2nd Friday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Tasks_\n",
    "1. Debugging Kickstarting\n",
    "    - Train the task on two task instead of four.\n",
    "2. Rendering\n",
    "    - Add wandb rendering support for evaluators\n",
    "3. Read Paper\n",
    "\n",
    "### _Notes:_\n",
    "\n",
    "With 240 actors, I can get ~1k4 sps (5.8 sps for single actor) for the actor steps, 44 sps for learner steps, \n",
    "\n",
    "### _Thoughts_\n",
    "\n",
    "During interleave training, the learner is getting experience from the reply buffer in a batched manner. I am not sure whether this is a good idea for multi-task training. The setup is a little bit different compared with CoMic style, since the task switching allow the learner to focus on single task during the gradient step.\n",
    "\n",
    "During joint training, the model seems to inevitably focus on a single task but not the other task. The ReplayBuffer now is a mixed buffer with experiences between tasks. I don't know how that affects the model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Rendering of Generalist - RunGaps_\n",
    "\n",
    "The following is trained by loading the checkpoint of the bowl escape, and run joint training on all four tasks.\n",
    "\n",
    "Definitely interesting that by loading the checkpoint from the bowl escape, the is walking on its foot but not the back. But it lost the ability to do jumps. I also notice that the rodent is awared of the exisitence of the gap, and it is staying on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"/root/vast/scott-yang/flybody/docs/videos/joint_training/run_gaps_genearlist_policy_80.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"/root/vast/scott-yang/flybody/docs/videos/joint_training/run_gaps_genearlist_policy_80.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meanwhile, the same model on rodent run gaps environments does not do well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"/root/vast/scott-yang/flybody/docs/videos/joint_training/bowl_escape_genearlist_policy_80.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"/root/vast/scott-yang/flybody/docs/videos/joint_training/bowl_escape_genearlist_policy_80.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rodent is very actively exploring/moving for the rest of the tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"/root/vast/scott-yang/flybody/docs/videos/joint_training/two_touch_genearlist_policy_80.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"/root/vast/scott-yang/flybody/docs/videos/joint_training/two_touch_genearlist_policy_80.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"/root/vast/scott-yang/flybody/docs/videos/joint_training/maze_forage_genearlist_policy_80.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"/root/vast/scott-yang/flybody/docs/videos/joint_training/maze_forage_genearlist_policy_80.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bowl Escape Checkpoint on Four Tasks\n",
    "\n",
    "Interestingly, when directly loading the checkpoints of bowl escape to perform all other tasks, it exhibits stereotypical behavior of walking and exploring, and casually hitting the goal some time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Tasks:_\n",
    "\n",
    "1. Implement multiple reply buffers for each tasks, and the learner of the gradient steps asks for experience in batch but in pure tasks.\n",
    "    - Helps a lot when doing the gradient updates, to preserves the gradient pureity\n",
    "\n",
    "_Current Issue:_\n",
    "\n",
    "Multiple Replay Server did not spwan successfully in the ray cluster. Need to debug, and look into the functional decoration of this object. Seems like it only keeps the last replay server for some unkown issue. Maybe need a `RemoteAsLocal` warppers too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aug 5, Monday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Issue:_ The training terminated because of OOM issue. Likely because of the replay buffer size being too large. Since we have 4 replay buffers at this point, we should limit the replay buffer size to be smaller.\n",
    "\n",
    "_Tasks:_ \n",
    "\n",
    "- Train rodent in only two tasks, to test the generalizability. \n",
    "- Understand how tf checkpoint system work, and how the `flybody` save and load the checkpoint\n",
    "\n",
    "_Thoughts:_\n",
    "\n",
    "From the Virtual Rodent Ethology paper, they mentioned:\n",
    "\n",
    "> Empirically, we found that the “escape” task was more challenging to learn during interleaved training relative to the other tasks.\n",
    "\n",
    "I also observed this trend in my multi-tasks training. When loading the checkpoint from the escape task to train the multi-tasks policy, the agent shows Catastrophic Forgetting (CT) for this specific task (the episode returns for the bowl escape evaluator collapse.)\n",
    "\n",
    "<img src=\"imgs/image.png\" alt=\"Image Description\" width=\"30%\" height=\"auto\"> \n",
    "\n",
    "\n",
    "_Good News!!!!_ Multiple replay servers achieves load balancing for each actors. I observed a 30% increase in CPU usage in node (40% -> 70%) and 60% increase in network activities (30 MB/s -> 52MB/s) by increasing the number of replay server from 1 to 2. From 2 to 3, we need to decrease the number of actor to accomondate more aggresive learner and more compute the replay server needs, this equates roughly 5 sps for each actors, and having 280 actors concurrently, it gets 1k4 sps for the environment steps. \n",
    "\n",
    "_Updates:_ Current setup can gives me 2.5k sps env steps.\n",
    "\n",
    "_Plans:_ Train the bowl escape expert further until it fully solve the task, and use kickstarting to train the generalist. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aug 6 Tuesday\n",
    "\n",
    "Implemented wandb logging for videos.\n",
    "\n",
    "Train generalist on four tasks based on the kickstarting for the bowl escape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aug 7 Wednesday\n",
    "\n",
    "Generalist training heavily flavors the `run-gaps` tasks. By inspecting the rendering, it seems like the rodent only flavors a single orientation of movements. Very strictly overfit to a very stereotypical movements, and damper all other signal down. In other words, the agent does not know how to recover itself back from the original state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aug 8 Thursday\n",
    "\n",
    "_SubGroup Meeting:_\n",
    "\n",
    "Talmo & Charles Suggested the following approches:\n",
    "\n",
    "1. Force the graident steps to be variable based on the task performace. Record the baseline performance and keep updating it until we observe improvements to the baseline task.\n",
    "2. Random sampling according to their task importance.\n",
    "3. Use encoder/decoder archtecture from the humanoid to test out the skill transfer.\n",
    "\n",
    "Personaly, I think the third approach is promising by direct skill transfer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Humanoid Imitation + Skill Transfer:_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aug 12 Monday\n",
    "\n",
    "_LT Goals:_\n",
    "\n",
    "1. Lab Meeting @ Aug 20th, Read relevant paper\n",
    "2. Imitation Learning\n",
    "3. Online Learning\n",
    "\n",
    "_ST Goals:_\n",
    "\n",
    "1. Clean up the API after merge\n",
    "1. Test train scripts for online & imitaion learning\n",
    "1. Explore checkpointing system for shared decoder arch,\n",
    "    - Partial load of the checkpoint\n",
    "    - with a small learning rate\n",
    "1. Multi-task training and skills preserving\n",
    "1. Formulate the training scripts and the amount of resources I need for training for the grant.\n",
    "\n",
    "_TODOs:_\n",
    "\n",
    "- [x] For rendering, remove the vision from the rendering.\n",
    "- [ ] Add KL penalty and variational network size for encoder and decoder arch.\n",
    "- [ ] Really Filter out the STAC dataset's quality. Some clip is just not useable for training.\n",
    "\n",
    "\n",
    "### _Notes:_\n",
    "\n",
    "For imitation learning, each environment loop takes around 4 GB for each actor. This is ~3x more memory intensive compared with online learning.\n",
    "\n",
    "_ReplayBuffer Param:_ Previously, the execution of the actor and the learner is rate limited by the replay buffer's rate limiter `samples_per_insert=15` (in the default setting). This indicates that each insert from the actor needs to be sample at least 15 times. For properly configured replay buffer, we don't need to use the multiple virtual server trick. it can use the full CPU resources.\n",
    "\n",
    "_Simulation Performance Metric:_ I can get consistent 9.5k sps for the humanoid tracking environment.\n",
    "\n",
    "\n",
    "_Progress!!!:_ The humanoid tracking learnt it really quick how to imitate the clip in 1 hrs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"/root/vast/scott-yang/flybody/training/ray-humanoid-imitation_humanoid-ckpts/b6043792-58eb-11ef-bc43-2ae1cc4cdff5/videos/imitation_humanoid-8.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"/root/vast/scott-yang/flybody/training/ray-humanoid-imitation_humanoid-ckpts/b6043792-58eb-11ef-bc43-2ae1cc4cdff5/videos/imitation_humanoid-8.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aug 13 Tuesday\n",
    "\n",
    "I trained the rodent imitation overnight, seems like that rodent of the STAC data is penetrating into the floor and having a rebounce from the floor. Current fix is that when initializing the episode, we provide an optional `offset` param to the function to shift the pose of the walker. Currently, I add 0.02 to the z axis of the walker..\n",
    "\n",
    "\n",
    "Additionally, seems like the simulation speed is really fast! Now it might be rate limited by the model update intervals, becasue the learner is constantly sending a large volumn of data out. Maybe reduce the frequency it shared the checkpoint to the actors.\n",
    "\n",
    "Yes! I think I ahve correctly identified the bottleneck, and now the simulation seems to be much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aug 14 Wednesday\n",
    "\n",
    "- [ ] Schedule training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aug 15 Thursday\n",
    "\n",
    "### _TODOs:_\n",
    "\n",
    "**Engineering**\n",
    "\n",
    "- [ ] implement the walker rescaling, and optimize the environment initialization by initializing episode without any contacts.\n",
    "    - [Reference, how dm_control initialize CMU humanoid](https://github.com/google-deepmind/dm_control/blob/main/dm_control/suite/humanoid_CMU.py#L136-L149)\n",
    "    - [Reference, rescale_walkers](https://github.com/google-deepmind/dm_control/blob/c7108371804ccbd1dd848b75d5877cbdd09e3a8a/dm_control/locomotion/walkers/rescale.py#L39-L60)\n",
    "- [ ] Implement better logging statistic for the evaluator\n",
    "    - Normalized the episode return.\n",
    "    - Multiple evaluator and report avg episode length across.\n",
    "    - Maybe actor can send summary statistics to a centralized stats.\n",
    "- [ ] Implement better clip choosing logic for more selective training\n",
    "    - look into the `reset()` function of the environment.\n",
    "- [ ] Dry test run with PPO.\n",
    "\n",
    "**Understanding**\n",
    "\n",
    "- [ ] Read the TL Paper, start drafting the presentation for next Tuesday.\n",
    "\n",
    "**Engineering Challengings**\n",
    "\n",
    "1. For `acme.EnvironmentLoop`, the run function did not return the result of a single episode, so sub-classing does not work by calling super.\n",
    "\n",
    "Sol 1: disable time filter of the logger. Easy to implement but hard to intepret the result\n",
    "\n",
    "Sol 2: let the evaluator has memory, so that it can reports averge over the minutes it spent.\n",
    "\n",
    "**Discoveries**\n",
    "\n",
    "- Lowering the batch size helps the training, as large batch size probably crush the gradient.\n",
    "- Termination threshold is CRITICAL to control explore & exploit. Low (_sensitive_) termination threshold will result in agent not able to explore enough, as the environment will constantly terminating after a single exploring steps. High (_insensitive_) termination threshold heavily favors exploiting the unrealistic positions, such as laying on the ground while the reference trajectory is moving. This contaminates the replay experience, as the most of the experience is laying down and do nothing. Fine tuning the termination threshold is key to make sure the learner has meaningful experience while also exploring unfamiliar landscape. This is becasue episode reward is highly correlated to the episode length -- as you live longer, you will receive more reward. The relationships between TT and exploration is an parabola, while exploitation is the inverted parabola.\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aug 16 Friday\n",
    "\n",
    "### _TODOs:_\n",
    "\n",
    "### Notes:\n",
    "\n",
    "Seems like the imitation learning task is very sensitive to the hyperparameter tuning. One change in the hyperparam will result in the moodel to forget / not able to learn the task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flybody",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
